{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copyright (c) 2017, by the Authors: Amir H. Abdi\n",
    "This software is freely available under the MIT Public License. \n",
    "Please see the License file in the root for details.\n",
    "\n",
    "The following code snippet will convert the keras model file,\n",
    "which is saved using model.save('kerasmodel_weight_file'),\n",
    "to the freezed .pb tensorflow weight file which holds both the \n",
    "network architecture and its associated weights.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input args:  Namespace(f='C:\\\\Users\\\\AHA\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-09592e76-8753-46e9-b1d5-137bd526405b.json', graph_def=False, input_fld='.', input_model_file='full_yolo_backend.h5', num_outputs=1, output_fld='.', output_graphdef_file='model.ascii', output_model_file='model.pb', output_node_prefix='output_node')\n"
     ]
    }
   ],
   "source": [
    "# setting input arguments\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='set input arguments')\n",
    "parser.add_argument('-input_fld', action=\"store\", \n",
    "                    dest='input_fld', type=str, default='.')\n",
    "\n",
    "parser.add_argument('-output_fld', action=\"store\", \n",
    "                    dest='output_fld', type=str, default='.')\n",
    "\n",
    "parser.add_argument('-input_model_file', action=\"store\", \n",
    "                    dest='input_model_file', type=str, default='full_yolo_backend.h5')\n",
    "\n",
    "parser.add_argument('-output_model_file', action=\"store\", \n",
    "                    dest='output_model_file', type=str, default='model.pb')\n",
    "\n",
    "parser.add_argument('-output_graphdef_file', action=\"store\", \n",
    "                    dest='output_graphdef_file', type=str, default='model.ascii')\n",
    "\n",
    "parser.add_argument('-num_outputs', action=\"store\", \n",
    "                    dest='num_outputs', type=int, default=1)\n",
    "\n",
    "parser.add_argument('-graph_def', action=\"store\", \n",
    "                    dest='graph_def', type=bool, default=False)\n",
    "\n",
    "parser.add_argument('-output_node_prefix', action=\"store\", \n",
    "                    dest='output_node_prefix', type=str, default='output_node')\n",
    "\n",
    "parser.add_argument('-f')\n",
    "args = parser.parse_args()\n",
    "print('input args: ', args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the following lines to alter the default values set above\n",
    "# args.input_fld = '.'\n",
    "# args.output_fld = '.'\n",
    "# args.input_model_file = 'model.h5'\n",
    "# args.output_model_file = 'model.pb'\n",
    "\n",
    "# num_output: this value has nothing to do with the number of classes, batch_size, etc., \n",
    "# and it is mostly equal to 1. \n",
    "# If you have a multi-stream network (forked network with multiple outputs), \n",
    "# set the value to the number of outputs.\n",
    "num_output = args.num_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import os.path as osp\n",
    "from keras import backend as K\n",
    "\n",
    "output_fld =  args.output_fld\n",
    "if not os.path.isdir(output_fld):\n",
    "    os.mkdir(output_fld)\n",
    "weight_file_path = osp.join(args.input_fld, args.input_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load keras model and rename output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output nodes names are:  ['output_node0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\keras\\models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "weight_file_path = 'test.h5'\n",
    "K.set_learning_phase(0)\n",
    "net_model = load_model(weight_file_path)\n",
    "\n",
    "pred = [None]*num_output\n",
    "pred_node_names = [None]*num_output\n",
    "for i in range(num_output):\n",
    "    pred_node_names[i] = args.output_node_prefix+str(i)\n",
    "    pred[i] = tf.identity(net_model.outputs[i], name=pred_node_names[i])\n",
    "print('output nodes names are: ', pred_node_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [optional] write graph definition in ascii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = K.get_session()\n",
    "\n",
    "if args.graph_def:\n",
    "    f = args.output_graphdef_file \n",
    "    tf.train.write_graph(sess.graph.as_graph_def(), output_fld, f, as_text=True)\n",
    "    print('saved the graph definition in ascii format at: ', osp.join(output_fld, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert variables to constants and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Transform 'quantize_weights' not recognized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-259c610a002a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_transforms\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTransformGraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtransforms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"quantize_weights\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtransformed_graph_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTransformGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_graph_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_node_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mconstant_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_variables_to_constants\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformed_graph_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_node_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mgraph_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconstant_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_fld\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_model_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\tools\\graph_transforms\\__init__.py\u001b[0m in \u001b[0;36mTransformGraph\u001b[1;34m(input_graph_def, inputs, outputs, transforms)\u001b[0m\n\u001b[0;32m     49\u001b[0m     output_graph_def_string = TransformGraphWithStringInputs(\n\u001b[0;32m     50\u001b[0m         \u001b[0minput_graph_def_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs_string\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         transforms_string, status)\n\u001b[0m\u001b[0;32m     52\u001b[0m   \u001b[0moutput_graph_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphDef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[0moutput_graph_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_graph_def_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    471\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Transform 'quantize_weights' not recognized."
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.python.framework import graph_io\n",
    "from tensorflow.tools.graph_transforms import TransformGraph\n",
    "transforms = [\"strip_unused_nodes\", \"quantize_weights\", \"quantize_nodes\"]\n",
    "transformed_graph_def = TransformGraph(sess.graph.as_graph_def(), [], pred_node_names, transforms)\n",
    "constant_graph = graph_util.convert_variables_to_constants(sess, transformed_graph_def, pred_node_names)\n",
    "graph_io.write_graph(constant_graph, output_fld, args.output_model_file, as_text=False)\n",
    "print('saved the freezed graph (ready for inference) at: ', osp.join(output_fld, args.output_model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 4 variables.\n",
      "Converted 4 variables to const ops.\n",
      "saved the freezed graph (ready for inference) at:  .\\model.pb\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.python.framework import graph_io\n",
    "constant_graph = graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(), pred_node_names)\n",
    "graph_io.write_graph(constant_graph, output_fld, args.output_model_file, as_text=False)\n",
    "print('saved the freezed graph (ready for inference) at: ', osp.join(output_fld, args.output_model_file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
